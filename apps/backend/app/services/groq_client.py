"""Groq API client for AI-powered features."""

import asyncio
import json
import logging
from typing import Dict, List, Optional, Any
import aiohttp
from tenacity import retry, stop_after_attempt, wait_exponential

from app.core.config import settings

logger = logging.getLogger(__name__)


class GroqClient:
    """Client for interacting with Groq API using Llama 70B model."""
    
    def __init__(self):
        self.api_key = settings.GROQ_API_KEY
        self.model = settings.GROQ_MODEL
        self.base_url = "https://api.groq.com/openai/v1"
        
        if not self.api_key:
            logger.warning("GROQ_API_KEY not set. AI features will be limited.")
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def generate_response(
        self,
        messages: List[Dict[str, str]],
        max_tokens: int = 1000,
        temperature: float = 0.7,
        top_p: float = 1.0,
        stream: bool = False
    ) -> str:
        """
        Generate response using Groq API.
        
        Args:
            messages: List of message dictionaries with 'role' and 'content'
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature (0-2)
            top_p: Nucleus sampling parameter
            stream: Whether to stream the response
            
        Returns:
            Generated response text
        """
        if not self.api_key:
            raise ValueError("Groq API key not configured")
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "stream": stream
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as response:
                    
                    if response.status != 200:
                        error_text = await response.text()
                        logger.error(f"Groq API error {response.status}: {error_text}")
                        raise Exception(f"Groq API error: {response.status}")
                    
                    result = await response.json()
                    
                    if "choices" not in result or not result["choices"]:
                        raise Exception("No response generated by Groq API")
                    
                    return result["choices"][0]["message"]["content"].strip()
                    
        except asyncio.TimeoutError:
            logger.error("Groq API request timed out")
            raise Exception("Request timed out")
        except Exception as e:
            logger.error(f"Error calling Groq API: {e}")
            raise
    
    async def generate_interview_questions(
        self,
        interview_type: str,
        category: str,
        difficulty_level: str,
        company_name: Optional[str] = None,
        position_title: Optional[str] = None,
        topic_tags: Optional[List[str]] = None,
        count: int = 5,
        previous_questions: Optional[List[str]] = None,
        user_performance: Optional[Dict[str, float]] = None
    ) -> List[Dict[str, Any]]:
        """
        Generate interview questions using AI.
        
        Args:
            interview_type: Type of interview (behavioral, technical, hr, etc.)
            category: Question category
            difficulty_level: Difficulty level (easy, medium, hard)
            company_name: Target company name
            position_title: Target position
            topic_tags: Relevant topics
            count: Number of questions to generate
            previous_questions: Previously asked questions to avoid repetition
            user_performance: User's performance data for adaptive questioning
            
        Returns:
            List of generated questions with metadata
        """
        
        # Build context-aware prompt
        prompt = self._build_question_generation_prompt(
            interview_type=interview_type,
            category=category,
            difficulty_level=difficulty_level,
            company_name=company_name,
            position_title=position_title,
            topic_tags=topic_tags or [],
            count=count,
            previous_questions=previous_questions or [],
            user_performance=user_performance
        )
        
        messages = [
            {
                "role": "system",
                "content": """You are an expert interview coach and recruiter with 15+ years of experience. 
                Generate realistic, engaging interview questions that are commonly asked by top companies. 
                Focus on practical scenarios and real-world applications. 
                Always return valid JSON format only."""
            },
            {
                "role": "user",
                "content": prompt
            }
        ]
        
        try:
            response = await self.generate_response(
                messages=messages,
                max_tokens=2000,
                temperature=0.7
            )
            
            # Parse JSON response
            questions_data = json.loads(response)
            
            # Validate and format questions
            questions = questions_data.get("questions", [])
            formatted_questions = []
            
            for i, q in enumerate(questions[:count]):
                formatted_question = {
                    "question_text": q.get("question_text", "").strip(),
                    "expected_duration": q.get("expected_duration", 120),
                    "context_information": q.get("context_information", ""),
                    "evaluation_criteria": q.get("evaluation_criteria", []),
                    "sample_answers": q.get("sample_answers", []),
                    "difficulty_adjustment": q.get("difficulty_adjustment", 0.0),
                    "generated_by_ai": True,
                    "generation_prompt": prompt[:500] + "..." if len(prompt) > 500 else prompt
                }
                
                # Validate required fields
                if formatted_question["question_text"]:
                    formatted_questions.append(formatted_question)
            
            logger.info(f"Generated {len(formatted_questions)} questions for {interview_type}/{category}")
            return formatted_questions
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON response from Groq: {e}")
            # Return fallback questions
            return self._generate_fallback_questions(interview_type, category, count)
        except Exception as e:
            logger.error(f"Error generating questions with Groq: {e}")
            return self._generate_fallback_questions(interview_type, category, count)
    
    def _build_question_generation_prompt(
        self,
        interview_type: str,
        category: str,
        difficulty_level: str,
        company_name: Optional[str],
        position_title: Optional[str],
        topic_tags: List[str],
        count: int,
        previous_questions: List[str],
        user_performance: Optional[Dict[str, float]]
    ) -> str:
        """Build detailed prompt for question generation."""
        
        # Base context
        context_parts = [
            f"Generate {count} {difficulty_level} level {interview_type} interview questions",
            f"Category: {category}",
        ]
        
        if company_name:
            context_parts.append(f"Company: {company_name}")
        
        if position_title:
            context_parts.append(f"Position: {position_title}")
        
        if topic_tags:
            context_parts.append(f"Topics: {', '.join(topic_tags)}")
        
        # Adaptive questioning based on performance
        if user_performance:
            avg_score = sum(user_performance.values()) / len(user_performance)
            if avg_score > 0.8:
                context_parts.append("User is performing well - increase difficulty slightly")
            elif avg_score < 0.5:
                context_parts.append("User is struggling - provide supportive, confidence-building questions")
        
        # Avoid repetition
        if previous_questions:
            context_parts.append(f"Avoid similar questions to: {'; '.join(previous_questions[:3])}")
        
        # Category-specific guidelines
        category_guidelines = {
            "behavioral": "Focus on STAR method scenarios, past experiences, and soft skills",
            "technical_coding": "Include algorithmic problems, data structures, and coding challenges",
            "technical_system_design": "Focus on scalability, architecture, and system components",
            "hr_general": "Cover motivation, culture fit, and career goals",
            "company_specific": "Include company values, products, and industry knowledge",
            "situational": "Present hypothetical scenarios and problem-solving situations"
        }
        
        if category in category_guidelines:
            context_parts.append(f"Guidelines: {category_guidelines[category]}")
        
        # Difficulty-specific instructions
        difficulty_instructions = {
            "easy": "Use straightforward questions suitable for entry-level candidates",
            "medium": "Include moderate complexity requiring some experience",
            "hard": "Create challenging questions for senior-level candidates"
        }
        
        if difficulty_level in difficulty_instructions:
            context_parts.append(f"Difficulty: {difficulty_instructions[difficulty_level]}")
        
        prompt = f"""
{' | '.join(context_parts)}

Requirements:
1. Questions should be realistic and commonly asked in actual interviews
2. Include specific, actionable evaluation criteria (3-5 points each)
3. Provide key answer points (not full answers) that interviewers look for
4. Estimate realistic response duration in seconds (60-300 typical range)
5. Add context information when helpful for understanding the question
6. Ensure questions are appropriate for the specified difficulty level
7. Make questions engaging and thought-provoking

Return ONLY valid JSON in this exact format:
{{
  "questions": [
    {{
      "question_text": "Your detailed question here",
      "expected_duration": 180,
      "context_information": "Background context if needed",
      "evaluation_criteria": [
        "Specific criterion 1",
        "Specific criterion 2", 
        "Specific criterion 3"
      ],
      "sample_answers": [
        "Key point candidates should mention",
        "Another important aspect",
        "Additional consideration"
      ],
      "difficulty_adjustment": 0.0
    }}
  ]
}}
"""
        
        return prompt.strip()
    
    def _generate_fallback_questions(
        self, 
        interview_type: str, 
        category: str, 
        count: int
    ) -> List[Dict[str, Any]]:
        """Generate fallback questions when AI generation fails."""
        
        fallback_templates = {
            "behavioral": [
                "Tell me about a time when you faced a significant challenge at work. How did you handle it?",
                "Describe a situation where you had to work with a difficult team member. What was your approach?",
                "Give me an example of when you had to learn something new quickly. How did you go about it?",
                "Tell me about a time when you made a mistake. How did you handle it?",
                "Describe a situation where you had to meet a tight deadline. What steps did you take?"
            ],
            "technical_coding": [
                "Implement a function to reverse a linked list.",
                "Design an algorithm to find the longest palindromic substring.",
                "Write code to implement a basic LRU cache.",
                "Solve the two-sum problem with optimal time complexity.",
                "Implement a function to validate a binary search tree."
            ],
            "hr_general": [
                "Why are you interested in this position?",
                "What are your greatest strengths and weaknesses?",
                "Where do you see yourself in 5 years?",
                "Why are you looking to leave your current role?",
                "What motivates you in your work?"
            ]
        }
        
        templates = fallback_templates.get(interview_type, fallback_templates["behavioral"])
        questions = []
        
        for i in range(min(count, len(templates))):
            questions.append({
                "question_text": templates[i],
                "expected_duration": 120,
                "context_information": "",
                "evaluation_criteria": ["Clarity", "Relevance", "Detail", "Structure"],
                "sample_answers": [],
                "difficulty_adjustment": 0.0,
                "generated_by_ai": False,
                "generation_prompt": "Fallback template"
            })
        
        return questions
    
    async def analyze_interview_response(
        self,
        question_text: str,
        response_text: str,
        question_category: str,
        evaluation_criteria: List[str],
        interview_context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Analyze interview response using AI.
        
        Args:
            question_text: The interview question
            response_text: Candidate's response
            question_category: Category of the question
            evaluation_criteria: Criteria for evaluation
            interview_context: Additional context about the interview
            
        Returns:
            Analysis results with scores and feedback
        """
        
        prompt = self._build_response_analysis_prompt(
            question_text=question_text,
            response_text=response_text,
            question_category=question_category,
            evaluation_criteria=evaluation_criteria,
            interview_context=interview_context
        )
        
        messages = [
            {
                "role": "system",
                "content": """You are an expert interview coach and evaluator. 
                Analyze candidate responses objectively and provide constructive feedback.
                Focus on both content quality and communication effectiveness.
                Always return valid JSON format only."""
            },
            {
                "role": "user",
                "content": prompt
            }
        ]
        
        try:
            response = await self.generate_response(
                messages=messages,
                max_tokens=1500,
                temperature=0.3
            )
            
            analysis_data = json.loads(response)
            
            # Validate and format analysis
            return {
                "content_relevance": max(0.0, min(1.0, analysis_data.get("content_relevance", 0.5))),
                "technical_accuracy": max(0.0, min(1.0, analysis_data.get("technical_accuracy", 0.5))),
                "structure_score": max(0.0, min(1.0, analysis_data.get("structure_score", 0.5))),
                "overall_score": max(0.0, min(100.0, analysis_data.get("overall_score", 50.0))),
                "communication_score": max(0.0, min(100.0, analysis_data.get("communication_score", 50.0))),
                "content_score": max(0.0, min(100.0, analysis_data.get("content_score", 50.0))),
                "feedback": analysis_data.get("feedback", "Analysis completed."),
                "improvement_suggestions": analysis_data.get("improvement_suggestions", []),
                "strengths": analysis_data.get("strengths", []),
                "weaknesses": analysis_data.get("weaknesses", [])
            }
            
        except Exception as e:
            logger.error(f"Error analyzing response with Groq: {e}")
            # Return default analysis
            return {
                "content_relevance": 0.5,
                "technical_accuracy": 0.5,
                "structure_score": 0.5,
                "overall_score": 50.0,
                "communication_score": 50.0,
                "content_score": 50.0,
                "feedback": "Analysis temporarily unavailable. Please try again.",
                "improvement_suggestions": [],
                "strengths": [],
                "weaknesses": []
            }
    
    def _build_response_analysis_prompt(
        self,
        question_text: str,
        response_text: str,
        question_category: str,
        evaluation_criteria: List[str],
        interview_context: Dict[str, Any]
    ) -> str:
        """Build prompt for response analysis."""
        
        prompt = f"""
Analyze this interview response comprehensively:

QUESTION: {question_text}
CATEGORY: {question_category}
EVALUATION CRITERIA: {', '.join(evaluation_criteria)}

CANDIDATE RESPONSE: {response_text}

INTERVIEW CONTEXT:
- Type: {interview_context.get('interview_type', 'General')}
- Company: {interview_context.get('company_name', 'Not specified')}
- Position: {interview_context.get('position_title', 'Not specified')}
- Difficulty Level: {interview_context.get('difficulty_level', 'Medium')}

ANALYSIS REQUIREMENTS:
1. Content Relevance (0-1): How well does the response address the question?
2. Technical Accuracy (0-1): Correctness of technical information (if applicable)
3. Structure Score (0-1): Organization, clarity, and logical flow
4. Overall Score (0-100): Comprehensive evaluation
5. Communication Score (0-100): Clarity, articulation, and presentation
6. Content Score (0-100): Depth, accuracy, and completeness of content

Provide specific, actionable feedback focusing on:
- What the candidate did well (strengths)
- Areas needing improvement (weaknesses)  
- Concrete suggestions for improvement

Return ONLY valid JSON in this format:
{{
  "content_relevance": 0.8,
  "technical_accuracy": 0.7,
  "structure_score": 0.9,
  "overall_score": 85.0,
  "communication_score": 80.0,
  "content_score": 90.0,
  "feedback": "Detailed constructive feedback here...",
  "improvement_suggestions": [
    "Specific suggestion 1",
    "Specific suggestion 2"
  ],
  "strengths": [
    "Specific strength 1", 
    "Specific strength 2"
  ],
  "weaknesses": [
    "Specific weakness 1",
    "Specific weakness 2"
  ]
}}
"""
        
        return prompt.strip()
    
    async def generate_followup_question(
        self,
        original_question: str,
        user_response: str,
        performance_score: float,
        interview_context: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """
        Generate adaptive follow-up question based on user response.
        
        Args:
            original_question: The original question asked
            user_response: User's response to analyze
            performance_score: Score from 0-1 indicating performance
            interview_context: Context about the interview
            
        Returns:
            Follow-up question or None if not needed
        """
        
        # Determine if follow-up is needed based on performance
        if performance_score > 0.8:
            # Good performance - ask a more challenging follow-up
            difficulty_adjustment = "increase difficulty"
        elif performance_score < 0.5:
            # Poor performance - ask a supportive follow-up
            difficulty_adjustment = "provide supportive guidance"
        else:
            # Average performance - clarifying follow-up
            difficulty_adjustment = "seek clarification or elaboration"
        
        prompt = f"""
Generate a follow-up interview question based on this interaction:

ORIGINAL QUESTION: {original_question}
CANDIDATE RESPONSE: {user_response}
PERFORMANCE SCORE: {performance_score:.2f}/1.0
ADJUSTMENT NEEDED: {difficulty_adjustment}

INTERVIEW CONTEXT:
- Type: {interview_context.get('interview_type', 'General')}
- Category: {interview_context.get('category', 'General')}
- Company: {interview_context.get('company_name', 'Not specified')}

Generate a natural follow-up question that:
1. Builds on the candidate's response
2. Adjusts difficulty appropriately based on performance
3. Maintains interview flow and engagement
4. Provides opportunity for deeper insight

Return ONLY valid JSON:
{{
  "question_text": "Your follow-up question here",
  "expected_duration": 90,
  "context_information": "Why this follow-up is relevant",
  "evaluation_criteria": ["Criterion 1", "Criterion 2"],
  "is_followup": true,
  "difficulty_adjustment": 0.2
}}

If no follow-up is needed, return: {{"question_text": null}}
"""
        
        try:
            response = await self.generate_response(
                messages=[
                    {
                        "role": "system", 
                        "content": "You are an expert interviewer. Generate natural, engaging follow-up questions."
                    },
                    {"role": "user", "content": prompt}
                ],
                max_tokens=800,
                temperature=0.6
            )
            
            followup_data = json.loads(response)
            
            if followup_data.get("question_text"):
                return {
                    "question_text": followup_data["question_text"],
                    "expected_duration": followup_data.get("expected_duration", 90),
                    "context_information": followup_data.get("context_information", ""),
                    "evaluation_criteria": followup_data.get("evaluation_criteria", []),
                    "is_followup": True,
                    "difficulty_adjustment": followup_data.get("difficulty_adjustment", 0.0),
                    "generated_by_ai": True
                }
            
            return None
            
        except Exception as e:
            logger.error(f"Error generating follow-up question: {e}")
            return None